[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Handmade Machine Learning Algorithms",
    "section": "",
    "text": "Below is a collection on Machine Learning algorithms coded from scratch. This project is an exploration of how common machine learning models work under the hood. By implementing these algorithms from scratch in Python, I aim to deepen my understanding of the mathematics power these techniques, while also visualizing their pros, cons, and differences!"
  },
  {
    "objectID": "main.html#ols-regression",
    "href": "main.html#ols-regression",
    "title": "Handmade Machine Learning Algorithms",
    "section": "OLS Regression",
    "text": "OLS Regression\n\nclass ols_regression():\n\n    # Initialize the class\n    def __init__(self):\n        pass       \n    \n    def fit(self, X, y):\n        '''Fit the regression to the X data via the OLS equation'''\n\n        # Add a leading colums of 1s to the X data to account for the bias term\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n\n        # Train the data on (X.T @ X)^(-1) @ X.T @ y\n        ols = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n        self.coef = ols[1:]\n        self.bias = ols[0]\n\n    def predict(self, X):\n        '''Predict new data with the trained coefficients and bias'''\n\n        # Check if the X data is 1D and reshape if needed\n        if X.ndim == 1:\n                    X = X.reshape(-1, 1) \n\n        # Make predictions by dotting the new data with the coefficients and adding the bias\n        self.predictions = X.dot(self.coef) + self.bias\n        \n        return self.predictions\n\nFor each algorithm, we build two practice datasets on which we fit the algorithm. These are a linear dataset and an ‘ugly’ dataset that is non-linear and contains significant noise. We then plot the training data and a projection grid for the trained algorithm to understand how it makes predictions and showcase it’s ability to ‘understand’ patterns in the underlying data.\n\nols = ols_regression()\n\nplot_3d_regression(ols, model_name='OLS', data_type='linear')\nplot_3d_regression(ols, model_name='OLS', data_type='ugly')\n\n\n                                                \n\n\n                                                \n\n\nWhile we can see that the OLS regression fits the linear dataset quite well, there is significant error when fitting to the non-linear data. As we can see in the prediction grid, this is because the regression only operates in a linear fashion, which does match the structure of the training data."
  },
  {
    "objectID": "main.html#gradient-descent-regression",
    "href": "main.html#gradient-descent-regression",
    "title": "Handmade Machine Learning Algorithms",
    "section": "Gradient Descent Regression",
    "text": "Gradient Descent Regression\nNext, we build a linear regression via gradient descent. Unlike OLS which is a closed form solution, gradient descent operates by calculating the gradient of the loss function (MSE) for any given set of regression weights, and systematically changing the weights in order to ‘move down’ the gradient and minimize the loss of predictions.\nWhile the final result should be the same as in OLS, they do not always align due to some randomness in attempting to find the absolute minimum of the loss function (although predictions are generally close enough). That said, Gradient Descent is a good option for larger datasets due to its lower computational requirements.\n\nclass GDRegression():\n    '''Simple class for performing linear regression via gradient descent. Note: this is a simple execution and does not include options for \n       regularization'''\n    def __init__(self, epochs, eta):\n        '''Initialize the Gradient Descent Regression Class'''\n        self.epochs = epochs\n        self.eta = eta\n\n    def fit(self, X, y, batch_size = \"TBD\"):\n        '''Train the Gradient Descent Regression Class'''\n\n        if batch_size == 'TBD':\n            batch_size = X.shape[0]\n\n\n        # Create random initialization for the bias and coefficients\n        bias = np.random.random()\n        coef = np.random.random(X.shape[1])\n\n        # Iterate through each epoch\n        for iter in range(self.epochs):\n            \n            indices = np.random.choice(X.shape[0], size=min(batch_size, len(y)), replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n\n            # Make predictions for the X data being trained on\n            y_hat = X_batch.dot(coef) + bias\n\n            # Calculate the derrivative WRT bias and coef given the predicions\n            derr_b = 2/X_batch.shape[0] * sum((y_hat - y_batch))\n            derr_c = 2/X_batch.shape[0] * X_batch.T.dot(y_hat - y_batch)\n\n            # Update the bias and the coef based on the derrivative\n            bias = bias - derr_b * self.eta\n            coef = coef - derr_c * self.eta\n\n        # Finalize the bias and coef\n        self.bias = bias\n        self.coef = coef\n\n    def predict(self, X):\n        '''Predict new data given the learned bias and coef'''\n        predictions = X.dot(self.coef) + self.bias\n        return predictions\n\n        \n\n\ngd_reg = GDRegression(epochs=10000, eta=.01)\nplot_3d_regression(gd_reg, 'Gradient Descent', data_type='linear')\nplot_3d_regression(gd_reg, 'Gradient Descent', data_type='ugly')\n\n                                                \n\n\n                                                \n\n\nAs expected, the results for the gradient descent regression are effectively the same as for the OLS! The prediction grid is fit very well to the linear dataset, but fails to understanad the non-linear nature of the ‘ugly’ data."
  },
  {
    "objectID": "main.html#knn-regression",
    "href": "main.html#knn-regression",
    "title": "Handmade Machine Learning Algorithms",
    "section": "KNN Regression",
    "text": "KNN Regression\nWe next build a K-Nearest Neighbors regressor. K-Nearest Neighbors works by taking the K closest (physically) points to any specific point in question, and predicting the average of those K points as the output. This works with the assumption that point close together are similar in nature.\n\nclass KNNRegressor():\n    '''A simple class for performing Nearest-Neighbors Regression. Note: this is a simple execution, and does not include options for\n       changes in distance calculation metrics beyond Euclidian distance.'''\n\n    def __init__(self, n_neighbors=5):\n        '''Initialize the regressor with a defined number of nearest neighbors'''\n        self.n_neighbors = n_neighbors\n\n    def fit(self, X, y):\n        '''Train the regressor by loading in all X and y data'''\n        self.X = X\n        self.y = y\n\n    def predict(self, X):\n        '''Make predictions based on the training data using euclidian distance'''\n        predictions = np.empty(0)\n\n        # For each test point...\n        for test_point in X:\n            # Calculate the distance between the test point and all training points\n            distances = np.linalg.norm(self.X - test_point, axis=1)\n\n            # Find the n_neighbors closest points\n            closest_points_indices = np.argsort(distances)[:self.n_neighbors]\n\n            # Use the mean of the closest points to formulate a predictions and append to the predictions array\n            prediction = mean(self.y[closest_points_indices])\n            predictions = np.append(predictions, prediction)\n\n        return predictions\n\n\nknn_regressor = KNNRegressor()\n\nplot_3d_regression(knn_regressor, \"K-Nearest Neighbors Regression\", data_type='linear')\nplot_3d_regression(knn_regressor, \"K-Nearest Neighbors Regression\", data_type='ugly')\n\n                                                \n\n\n                                                \n\n\nWe first see that unilke in the linear regression, our prediction grids are now bumpy! This is because each prediction is dependant on the K nearest points, which introduces randomness into the prediction grid. The next big change vs. the previous linear regression is that the KNN regression is able to fit the non-linear data much better. However, we do still see some issues in fitting some of the very noisy data points well, as they fail to fit the underlying assumption that points close together are similar."
  },
  {
    "objectID": "main.html#decision-tree-regressor",
    "href": "main.html#decision-tree-regressor",
    "title": "Handmade Machine Learning Algorithms",
    "section": "Decision Tree Regressor",
    "text": "Decision Tree Regressor\nWe next build a decision tree regressor. Decision Tree Regression splits the data into subsets based on feature values to create a tree-like model of decisions. At each split, the algorithm chooses the feature and threshold that minimize the error in predicting the target variable.\nThe model predicts the target value of a new data point by following the tree’s splits until reaching the bottom of the tree , where the output is the average of the target values in that leaf.\n\nclass DecisionTreeRegressor():\n    '''A class to build a decision tree regressor. Note this is a simple executuion and does not include common parameters for regularization\n       other than max_depth'''\n    \n    def __init__(self, max_depth=None):\n        # Initializes the decision tree regressor with an optional maximum depth for the tree.\n        self.max_depth = max_depth\n\n    # Function for calculating the Mean Squared Error (MSE) of a split\n    def mse(self, y):\n        # MSE is calculated as the average of squared differences from the mean\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    # Function to find the best split at any given point (based on MSE)\n    def _best_split(self, X, y):\n        # Initialize variables to track the best split found\n        self.best_mse = float('inf')  # Best MSE starts as infinity\n        self.best_feature = None\n        self.best_split_value = None\n        self.best_left_y = None\n        self.best_right_y = None\n\n        # Loop over each feature in the dataset\n        for feature_num in range(X.shape[1]):\n            # Get unique values in the feature column to test potential splits\n            feature_values = np.unique(X[:, feature_num])\n            \n            # For each unique value, try splitting the data\n            for value in feature_values:\n                # Find indices where the feature values are less than or equal to the split value\n                left_index = X[:, feature_num] &lt;= value\n                right_index = X[:, feature_num] &gt; value\n\n                # Split the target values accordingly\n                left_targets = y[left_index]\n                right_targets = y[right_index]\n\n                # Proceed only if both splits result in non-empty groups\n                if len(left_targets) &gt; 0 and len(right_targets) &gt; 0:\n                    # Compute MSE for both the left and right splits\n                    left_mse = self.mse(left_targets)\n                    right_mse = self.mse(right_targets)\n                    \n                    # Calculate the weighted average MSE of the two splits\n                    total_average_mse = left_mse * len(left_targets)/len(y) + right_mse * len(right_targets)/len(y)\n\n                    # If this split provides a better (lower) MSE, update the best split found\n                    if total_average_mse &lt; self.best_mse:\n                        self.best_mse = total_average_mse\n                        self.best_feature = feature_num\n                        self.best_split_value = value\n                        self.left_y = left_targets\n                        self.right_y = right_targets\n\n        # Return the best split information (feature index, value, and target splits)\n        return self.best_split_value, self.best_feature, self.left_y, self.right_y\n    \n    # Function to recursively build the decision tree\n    def _build_tree(self, X, y, depth=0):\n        # Base case: If all targets are the same or max depth is reached, return the mean of the target values\n        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth):\n            return np.mean(y)\n        \n        # Find the best split for the data\n        best_split_value, best_feature, left_y, right_y = self._best_split(X, y)\n\n        # If no valid split was found, return the mean of the targets\n        if best_feature is None:\n            return np.mean(y)\n        \n        # Split the data based on the best feature and split value\n        left_index = X[:, best_feature] &lt;= best_split_value\n        right_index = X[:, best_feature] &gt; best_split_value\n\n        # Recursively build the left and right subtrees\n        left_tree = self._build_tree(X[left_index], left_y, depth + 1)\n        right_tree = self._build_tree(X[right_index], right_y, depth + 1)\n\n        # Return the current node as a dictionary with the best split and its left and right subtrees\n        return {\n            'feature': best_feature,\n            'value': best_split_value,\n            'left': left_tree,\n            'right': right_tree\n        }\n    \n    # Function to make a prediction for a single sample using the tree\n    def _single_prediction(self, tree, x):\n        # If the current tree node is a dictionary (not a leaf), recursively traverse the tree\n        if isinstance(tree, dict):\n            if x[tree['feature']] &lt; tree['value']:\n                return self._single_prediction(tree['left'], x)  # Go left\n            else:\n                return self._single_prediction(tree['right'], x)  # Go right\n        # If the current node is a leaf (not a dictionary), return the prediction (mean of targets)\n        else:\n            return tree\n        \n    # Function to predict target values for a set of samples\n    def predict(self, X):\n        # For each sample in X, make a prediction by recursively traversing the tree\n        predictions = np.array([self._single_prediction(self.tree, x) for x in X])\n        return predictions\n\n    # Function to fit the decision tree to the training data\n    def fit(self, X, y):\n        # Build the tree by calling the recursive function with the training data\n        self.tree = self._build_tree(X, y)\n\n\ndt_reg = DecisionTreeRegressor()\nplot_3d_regression(dt_reg, \"Decision Tree\", data_type='linear')\nplot_3d_regression(dt_reg, \"Decision Tree\", data_type='ugly')\n\n                                                \n\n\n                                                \n\n\nWith the Decision Tree regressor, similar to the KNN regressor, we are able to fit well to both the linear and non-linear datasets. The regression surface does however look a bit different, as there are sharper horozontal cuts rather than an overall bumpy surface. This is caused by the ‘cutting’ of the dataset on feature values during training, which segments the data sharply."
  },
  {
    "objectID": "main.html#random-forest-regression",
    "href": "main.html#random-forest-regression",
    "title": "Handmade Machine Learning Algorithms",
    "section": "Random Forest Regression",
    "text": "Random Forest Regression\nWe next build a Random Forest Regressor. Random Forest Regression improves upon Decision Trees by combining the predictions of multiple trees to make a more accurate and stable model. Each tree is trained on a random subset of the data and features, and the final prediction is the average of all the trees’ outputs.\n\nclass RandomForestRegressor():\n    def __init__(self, n_estimators, max_depth=None):\n        '''A class to build a Random Forest Regressor. Note this is a simple execution and does not include parameters for regularization\n           other than n_estimators and max_depth'''\n        \n        # Initializes the random forest regressor with the number of estimators (trees) and an optional maximum depth for each tree.\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n\n    # Function for calculating the Mean Squared Error (MSE) of a split\n    def mse(self, y):\n        # MSE is calculated as the average of squared differences from the mean\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    # Function to find the best split at any given point (based on MSE)\n    def _best_split(self, X, y):\n        # Initialize variables to track the best split found\n        self.best_mse = float('inf')  # Best MSE starts as infinity\n        self.best_feature = None\n        self.best_split_value = None\n        self.best_left_y = None\n        self.best_right_y = None\n\n        # Randomly sample 1/3 of the total features to consider for splitting\n        n_features_to_consider = max(1, X.shape[1] // 3)\n        random_feature_indices = np.random.choice(X.shape[1], size=n_features_to_consider, replace=False)\n\n        # Only consider the randomly selected features for splitting\n        feature_subset = X[:, random_feature_indices]\n\n        # Loop through the randomly selected features and find the best split\n        for i, feature_num in enumerate(random_feature_indices):  # Map back to original feature indices\n            feature_values = np.unique(feature_subset[:, i])\n            for value in feature_values:\n                # Boolean indices based on the feature subset\n                left_index = feature_subset[:, i] &lt;= value\n                right_index = feature_subset[:, i] &gt; value\n\n                # Subset targets using these indices\n                left_targets = y[left_index]\n                right_targets = y[right_index]\n\n                # Ensure both sides have samples\n                if len(left_targets) &gt; 0 and len(right_targets) &gt; 0:\n                    # Calculate the MSE for both the left and right subsets\n                    left_mse = self.mse(left_targets)\n                    right_mse = self.mse(right_targets)\n                    total_average_mse = left_mse * len(left_targets) / len(y) + right_mse * len(right_targets) / len(y)\n\n                    # If this split results in a better (lower) MSE, update the best split\n                    if total_average_mse &lt; self.best_mse:\n                        self.best_mse = total_average_mse\n                        self.best_feature = feature_num\n                        self.best_split_value = value\n                        self.left_y = left_targets\n                        self.right_y = right_targets\n\n        # Return the best split information (feature index, value, and target splits)\n        return self.best_split_value, self.best_feature, self.left_y, self.right_y\n\n    \n    # Function to recursively build the decision tree\n    def _build_tree(self, X, y, depth=0):\n        # Base case: If all targets are the same or max depth is reached, return the mean of the target values\n        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth):\n            return np.mean(y)\n        \n        # Find the best split for the data\n        best_split_value, best_feature, left_y, right_y = self._best_split(X, y)\n\n        # If no valid split was found, return the mean of the targets\n        if best_feature is None:\n            return np.mean(y)\n        \n        # Split the data based on the best feature and split value\n        left_index = X[:, best_feature] &lt;= best_split_value\n        right_index = X[:, best_feature] &gt; best_split_value\n\n        # Recursively build the left and right subtrees\n        left_tree = self._build_tree(X[left_index], left_y, depth + 1)\n        right_tree = self._build_tree(X[right_index], right_y, depth + 1)\n\n        # Return the current node as a dictionary with the best split and its left and right subtrees\n        return {\n            'feature': best_feature,\n            'value': best_split_value,\n            'left': left_tree,\n            'right': right_tree\n        }\n\n    # Function to make a prediction for a single sample using the tree\n    def _single_prediction(self, tree, x):\n        # If the current tree node is a dictionary (not a leaf), recursively traverse the tree\n        if isinstance(tree, dict):\n            if x[tree['feature']] &lt; tree['value']:\n                return self._single_prediction(tree['left'], x)  # Go left\n            else:\n                return self._single_prediction(tree['right'], x)  # Go right\n        # If the current node is a leaf (not a dictionary), return the prediction (mean of targets)\n        else:\n            return tree\n        \n    # Function to predict target values for a set of samples\n    def predict(self, X):\n        predictions = []\n        # For each tree in the random forest, make predictions for all samples in X\n        for tree in self.trees:\n            model_predictions = np.array([self._single_prediction(tree, x) for x in X])\n            predictions.append(model_predictions)\n        \n        # Combine all the predictions of all the trees and average across the trees\n        all_predictions = np.column_stack(predictions)\n        all_predictions = np.mean(all_predictions, axis=1)\n\n        return all_predictions\n        \n\n    # Function to train the random forest model\n    def fit(self, X, y):\n        models = []\n        # Create n decision trees, each trained with bootstrapping (sampling with replacement)\n        for n in range(self.n_estimators):\n            random_row_indices = np.random.choice(len(X), len(X), replace=True)\n            subset_X = X[random_row_indices]\n            subset_y = y[random_row_indices]\n            tree = self._build_tree(subset_X, subset_y)\n\n            # Add each trained tree to the list of models\n            models.append(tree)\n\n        # Save all the trained trees\n        self.trees = models\n\n\nrf_regressor = RandomForestRegressor(10)\nplot_3d_regression(rf_regressor, \"Random Forest Regressor\", \"linear\")\nplot_3d_regression(rf_regressor, \"Random Forest Regressor\", \"ugly\")\n\n                                                \n\n\n                                                \n\n\nAs expected, the Random Forest regressor fits both datasets quite well, and has a regression surface that is similar to the Decision Tree, although a bit smoother. Random Forest regression is likely to grow its improvements over Decision Trees as the underlying dataset grows larger and more complex."
  },
  {
    "objectID": "main.html#gradient-boosting-regression",
    "href": "main.html#gradient-boosting-regression",
    "title": "Handmade Machine Learning Algorithms",
    "section": "Gradient Boosting Regression",
    "text": "Gradient Boosting Regression\nFor our final regression method, we build a Gradient Boost Regressor. Gradient Boosting Regression builds a model by combining many small and poor decision trees, each one correcting the errors of the previous trees. It works iteratively, adding trees that focus on reducing the residual errors in the predictions. The final prediction is the sum of all the trees’ outputs.\n\nclass GradientBoostingRegressor:\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        '''A class for building a Gradient Boosting Regressor. Note this is a simple execution and does not include parameters for \n           regularization other than n_estimators, learning_rate, and max_depth'''\n\n        # Initialize model with given number of estimators, learning rate, and max depth for each tree\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n\n    def fit(self, X, y):\n        # Initialize predictions as the mean of the target values\n        self.y_pred = np.full_like(y, np.mean(y), dtype=np.float32)\n        self.trees = []\n\n        for _ in range(self.n_estimators):\n            # Calculate residuals\n            residuals = y - self.y_pred\n\n            # Fit a tree to the residuals\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residuals)\n\n            # Update predictions with the tree's output scaled by learning rate\n            self.y_pred += self.learning_rate * tree.predict(X)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        # Start with initial predictions as the mean of target values\n        y_pred = np.full(X.shape[0], np.mean(self.y_pred), dtype=np.float32)\n\n        # Sum the residual predictions from all trees\n        for tree in self.trees:\n            y_pred += self.learning_rate * tree.predict(X)\n        return y_pred\n\n\nboost_regressor = GradientBoostingRegressor(n_estimators=50)\nplot_3d_regression(boost_regressor, \"Gradient Boost Regressor\", \"linear\")\nplot_3d_regression(boost_regressor, \"Gradient Boost Regressor\", \"ugly\")\n\n                                                \n\n\n                                                \n\n\nWe once again see a very strong fit to the training data both in the linear and non-linear cases from the Gradient Boosting Regressor. Additionally, as compared to previous non-linear methods, the surface is a fair bit smoother. There are a few large, sharp cuts that represent overfitting, but these can be combatted with regularization techniques not included in this simple execution!"
  }
]