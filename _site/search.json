[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statistics import mean\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nHelper Functions\nimport plotly.graph_objects as go\n\ndef plot_3d_regression(model, model_name, data_type = 'linear'):\n\n    # Make Standard Data\n    X, y = make_regression(\n    n_samples=300,     # Number of samples\n    n_features=2,      # Number of features\n    noise=15,          # Add noise to make it realistic\n    random_state=42    # Set seed for reproducibility\n)\n\n    # Make standard non-linear data\n    if data_type.lower() == 'non-linear':\n        y = y + 500 * np.sin(X[:, 0]) + 250 * np.cos(X[:, 1])\n\n    # Make non-standard data\n    if data_type == 'ugly':\n        # Add non-linear transformations\n        y = y + 500 * np.sin(X[:, 0]) * np.cos(X[:, 1]) \\\n              + 300 * (X[:, 0] ** 2) \\\n              - 200 * np.exp(-X[:, 1] ** 2)\n\n        # Add random feature interactions\n        y = y + 100 * (X[:, 0] * X[:, 1])\n\n        # Add random noise to make it \"uglier\"\n        y = y + np.random.normal(0, 150, size=y.shape)\n\n    model.fit(X, y)\n\n\n    # Create a mesh grid for the features\n    x_grid, y_grid = np.meshgrid(np.linspace(min(X[:, 0]), max(X[:, 0]), 100),\n                                np.linspace(min(X[:, 1]), max(X[:, 1]), 100))\n    grid = np.vstack((x_grid.flatten(), y_grid.flatten())).T \n\n    predictions = model.predict(grid)\n\n    # Create 3D scatter plot for training data\n    scatter = go.Scatter3d(\n        x=X[:, 0], y=X[:, 1], z=y,\n        mode='markers', marker=dict(color='blue', size=5), name='Training Data'\n    )\n\n    # Create 3D surface plot for the regression surface\n    surface = go.Surface(\n        x=x_grid, y=y_grid, z=predictions.reshape(x_grid.shape), opacity=0.5, colorscale='Viridis', name='Regression Surface'\n    )\n\n    # Combine both traces into one figure\n    fig = go.Figure(data=[scatter, surface])\n\n    # Update layout for better visualization\n    fig.update_layout(\n        title=f'Training Data and Regression Surface for {model_name}',\n        scene=dict(\n            xaxis_title='Feature 1',\n            yaxis_title='Feature 2',\n            zaxis_title='Target'\n        )\n    )\n\n    # Show plot\n    fig.show()"
  },
  {
    "objectID": "main.html#ols",
    "href": "main.html#ols",
    "title": "Linear Regression",
    "section": "OLS",
    "text": "OLS\n\nclass ols_regression():\n\n    # Initialize the class\n    def __init__(self):\n        pass       \n    \n    def fit(self, X, y):\n        '''Fit the regression to the X data via the OLS equation'''\n\n        # Add a leading colums of 1s to the X data to account for the bias term\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n\n        # Train the data on (X.T @ X)^(-1) @ X.T @ y\n        ols = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n        self.coef = ols[1:]\n        self.bias = ols[0]\n\n    def predict(self, X):\n        '''Predict new data with the trained coefficients and bias'''\n\n        # Check if the X data is 1D and reshape if needed\n        if X.ndim == 1:\n                    X = X.reshape(-1, 1) \n\n        # Make predictions by dotting the new data with the coefficients and adding the bias\n        self.predictions = X.dot(self.coef) + self.bias\n        \n        return self.predictions\n\n\nols = ols_regression()\n\nplot_3d_regression(ols, model_name='OLS', data_type='linear')\nplot_3d_regression(ols, model_name='OLS', data_type='ugly')\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "main.html#gradient-descent-regression",
    "href": "main.html#gradient-descent-regression",
    "title": "Linear Regression",
    "section": "Gradient Descent Regression",
    "text": "Gradient Descent Regression\n\nclass GDRegression():\n    def __init__(self, epochs, eta):\n        '''Initialize the Gradient Descent Regression Class'''\n        self.epochs = epochs\n        self.eta = eta\n\n    def fit(self, X, y, batch_size = \"TBD\"):\n        '''Train the Gradient Descent Regression Class'''\n\n        if batch_size == 'TBD':\n            batch_size = X.shape[0]\n\n\n        # Create random initialization for the bias and coefficients\n        bias = np.random.random()\n        coef = np.random.random(X.shape[1])\n\n        # Iterate through each epoch\n        for iter in range(self.epochs):\n            \n            indices = np.random.choice(X.shape[0], size=min(batch_size, len(y)), replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n\n            # Make predictions for the X data being trained on\n            y_hat = X_batch.dot(coef) + bias\n\n            # Calculate the derrivative WRT bias and coef given the predicions\n            derr_b = 2/X_batch.shape[0] * sum((y_hat - y_batch))\n            derr_c = 2/X_batch.shape[0] * X_batch.T.dot(y_hat - y_batch)\n\n            # Update the bias and the coef based on the derrivative\n            bias = bias - derr_b * self.eta\n            coef = coef - derr_c * self.eta\n\n        # Finalize the bias and coef\n        self.bias = bias\n        self.coef = coef\n\n    def predict(self, X):\n        '''Predict new data given the learned bias and coef'''\n        predictions = X.dot(self.coef) + self.bias\n        return predictions\n\n        \n\n\ngd_reg = GDRegression(epochs=10000, eta=.01)\nplot_3d_regression(gd_reg, 'Gradient Descent', data_type='linear')\nplot_3d_regression(gd_reg, 'Gradient Descent', data_type='ugly')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "main.html#knn-regression",
    "href": "main.html#knn-regression",
    "title": "Linear Regression",
    "section": "KNN Regression",
    "text": "KNN Regression\n\nclass KNNRegressor():\n    def __init__(self, n_neighbors=5):\n        '''Initialize the regressor with a defined number of nearest neighbors'''\n        self.n_neighbors = n_neighbors\n\n    def fit(self, X, y):\n        '''Train the regressor by loading in all X and y data'''\n        self.X = X\n        self.y = y\n\n    def predict(self, X):\n        '''Make predictions based on the training data using euclidian distance'''\n        predictions = np.empty(0)\n\n        # For each test point...\n        for test_point in X:\n            # Calculate the distance between the test point and all training points\n            distances = np.linalg.norm(self.X - test_point, axis=1)\n\n            # Find the n_neighbors closest points\n            closest_points_indices = np.argsort(distances)[:self.n_neighbors]\n\n            # Use the mean of the closest points to formulate a predictions and append to the predictions array\n            prediction = mean(self.y[closest_points_indices])\n            predictions = np.append(predictions, prediction)\n\n        return predictions\n\n\nknn_regressor = KNNRegressor()\n\nplot_3d_regression(knn_regressor, \"K-Nearest Neighbors Regression\", data_type='linear')\nplot_3d_regression(knn_regressor, \"K-Nearest Neighbors Regression\", data_type='ugly')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nclass DecisionTreeRegressor():\n    def __init__(self, max_depth=None):\n        # Initializes the decision tree regressor with an optional maximum depth for the tree.\n        self.max_depth = max_depth\n\n    # Function for calculating the Mean Squared Error (MSE) of a split\n    def mse(self, y):\n        # MSE is calculated as the average of squared differences from the mean\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    # Function to find the best split at any given point (based on MSE)\n    def _best_split(self, X, y):\n        # Initialize variables to track the best split found\n        self.best_mse = float('inf')  # Best MSE starts as infinity\n        self.best_feature = None\n        self.best_split_value = None\n        self.best_left_y = None\n        self.best_right_y = None\n\n        # Loop over each feature in the dataset\n        for feature_num in range(X.shape[1]):\n            # Get unique values in the feature column to test potential splits\n            feature_values = np.unique(X[:, feature_num])\n            \n            # For each unique value, try splitting the data\n            for value in feature_values:\n                # Find indices where the feature values are less than or equal to the split value\n                left_index = X[:, feature_num] &lt;= value\n                right_index = X[:, feature_num] &gt; value\n\n                # Split the target values accordingly\n                left_targets = y[left_index]\n                right_targets = y[right_index]\n\n                # Proceed only if both splits result in non-empty groups\n                if len(left_targets) &gt; 0 and len(right_targets) &gt; 0:\n                    # Compute MSE for both the left and right splits\n                    left_mse = self.mse(left_targets)\n                    right_mse = self.mse(right_targets)\n                    \n                    # Calculate the weighted average MSE of the two splits\n                    total_average_mse = left_mse * len(left_targets)/len(y) + right_mse * len(right_targets)/len(y)\n\n                    # If this split provides a better (lower) MSE, update the best split found\n                    if total_average_mse &lt; self.best_mse:\n                        self.best_mse = total_average_mse\n                        self.best_feature = feature_num\n                        self.best_split_value = value\n                        self.left_y = left_targets\n                        self.right_y = right_targets\n\n        # Return the best split information (feature index, value, and target splits)\n        return self.best_split_value, self.best_feature, self.left_y, self.right_y\n    \n    # Function to recursively build the decision tree\n    def _build_tree(self, X, y, depth=0):\n        # Base case: If all targets are the same or max depth is reached, return the mean of the target values\n        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth):\n            return np.mean(y)\n        \n        # Find the best split for the data\n        best_split_value, best_feature, left_y, right_y = self._best_split(X, y)\n\n        # If no valid split was found, return the mean of the targets\n        if best_feature is None:\n            return np.mean(y)\n        \n        # Split the data based on the best feature and split value\n        left_index = X[:, best_feature] &lt;= best_split_value\n        right_index = X[:, best_feature] &gt; best_split_value\n\n        # Recursively build the left and right subtrees\n        left_tree = self._build_tree(X[left_index], left_y, depth + 1)\n        right_tree = self._build_tree(X[right_index], right_y, depth + 1)\n\n        # Return the current node as a dictionary with the best split and its left and right subtrees\n        return {\n            'feature': best_feature,\n            'value': best_split_value,\n            'left': left_tree,\n            'right': right_tree\n        }\n    \n    # Function to make a prediction for a single sample using the tree\n    def _single_prediction(self, tree, x):\n        # If the current tree node is a dictionary (not a leaf), recursively traverse the tree\n        if isinstance(tree, dict):\n            if x[tree['feature']] &lt; tree['value']:\n                return self._single_prediction(tree['left'], x)  # Go left\n            else:\n                return self._single_prediction(tree['right'], x)  # Go right\n        # If the current node is a leaf (not a dictionary), return the prediction (mean of targets)\n        else:\n            return tree\n        \n    # Function to predict target values for a set of samples\n    def predict(self, X):\n        # For each sample in X, make a prediction by recursively traversing the tree\n        predictions = np.array([self._single_prediction(self.tree, x) for x in X])\n        return predictions\n\n    # Function to fit the decision tree to the training data\n    def fit(self, X, y):\n        # Build the tree by calling the recursive function with the training data\n        self.tree = self._build_tree(X, y)\n\n\ndt_reg = DecisionTreeRegressor()\nplot_3d_regression(dt_reg, \"Decision Tree\", data_type='linear')\nplot_3d_regression(dt_reg, \"Decision Tree\", data_type='ugly')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "main.html#random-forest-regression",
    "href": "main.html#random-forest-regression",
    "title": "Linear Regression",
    "section": "Random Forest Regression",
    "text": "Random Forest Regression\n\nclass RandomForestRegressor():\n    def __init__(self, n_estimators, max_depth=None):\n        # Initializes the random forest regressor with the number of estimators (trees) and an optional maximum depth for each tree.\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n\n    # Function for calculating the Mean Squared Error (MSE) of a split\n    def mse(self, y):\n        # MSE is calculated as the average of squared differences from the mean\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    # Function to find the best split at any given point (based on MSE)\n    def _best_split(self, X, y):\n        # Initialize variables to track the best split found\n        self.best_mse = float('inf')  # Best MSE starts as infinity\n        self.best_feature = None\n        self.best_split_value = None\n        self.best_left_y = None\n        self.best_right_y = None\n\n        # Randomly sample 1/3 of the total features to consider for splitting\n        n_features_to_consider = max(1, X.shape[1] // 3)\n        random_feature_indices = np.random.choice(X.shape[1], size=n_features_to_consider, replace=False)\n\n        # Only consider the randomly selected features for splitting\n        feature_subset = X[:, random_feature_indices]\n\n        # Loop through the randomly selected features and find the best split\n        for i, feature_num in enumerate(random_feature_indices):  # Map back to original feature indices\n            feature_values = np.unique(feature_subset[:, i])\n            for value in feature_values:\n                # Boolean indices based on the feature subset\n                left_index = feature_subset[:, i] &lt;= value\n                right_index = feature_subset[:, i] &gt; value\n\n                # Subset targets using these indices\n                left_targets = y[left_index]\n                right_targets = y[right_index]\n\n                # Ensure both sides have samples\n                if len(left_targets) &gt; 0 and len(right_targets) &gt; 0:\n                    # Calculate the MSE for both the left and right subsets\n                    left_mse = self.mse(left_targets)\n                    right_mse = self.mse(right_targets)\n                    total_average_mse = left_mse * len(left_targets) / len(y) + right_mse * len(right_targets) / len(y)\n\n                    # If this split results in a better (lower) MSE, update the best split\n                    if total_average_mse &lt; self.best_mse:\n                        self.best_mse = total_average_mse\n                        self.best_feature = feature_num\n                        self.best_split_value = value\n                        self.left_y = left_targets\n                        self.right_y = right_targets\n\n        # Return the best split information (feature index, value, and target splits)\n        return self.best_split_value, self.best_feature, self.left_y, self.right_y\n\n    \n    # Function to recursively build the decision tree\n    def _build_tree(self, X, y, depth=0):\n        # Base case: If all targets are the same or max depth is reached, return the mean of the target values\n        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth):\n            return np.mean(y)\n        \n        # Find the best split for the data\n        best_split_value, best_feature, left_y, right_y = self._best_split(X, y)\n\n        # If no valid split was found, return the mean of the targets\n        if best_feature is None:\n            return np.mean(y)\n        \n        # Split the data based on the best feature and split value\n        left_index = X[:, best_feature] &lt;= best_split_value\n        right_index = X[:, best_feature] &gt; best_split_value\n\n        # Recursively build the left and right subtrees\n        left_tree = self._build_tree(X[left_index], left_y, depth + 1)\n        right_tree = self._build_tree(X[right_index], right_y, depth + 1)\n\n        # Return the current node as a dictionary with the best split and its left and right subtrees\n        return {\n            'feature': best_feature,\n            'value': best_split_value,\n            'left': left_tree,\n            'right': right_tree\n        }\n\n    # Function to make a prediction for a single sample using the tree\n    def _single_prediction(self, tree, x):\n        # If the current tree node is a dictionary (not a leaf), recursively traverse the tree\n        if isinstance(tree, dict):\n            if x[tree['feature']] &lt; tree['value']:\n                return self._single_prediction(tree['left'], x)  # Go left\n            else:\n                return self._single_prediction(tree['right'], x)  # Go right\n        # If the current node is a leaf (not a dictionary), return the prediction (mean of targets)\n        else:\n            return tree\n        \n    # Function to predict target values for a set of samples\n    def predict(self, X):\n        predictions = []\n        # For each tree in the random forest, make predictions for all samples in X\n        for tree in self.trees:\n            model_predictions = np.array([self._single_prediction(tree, x) for x in X])\n            predictions.append(model_predictions)\n        \n        # Combine all the predictions of all the trees and average across the trees\n        all_predictions = np.column_stack(predictions)\n        all_predictions = np.mean(all_predictions, axis=1)\n\n        return all_predictions\n        \n\n    # Function to train the random forest model\n    def fit(self, X, y):\n        models = []\n        # Create n decision trees, each trained with bootstrapping (sampling with replacement)\n        for n in range(self.n_estimators):\n            random_row_indices = np.random.choice(len(X), len(X), replace=True)\n            subset_X = X[random_row_indices]\n            subset_y = y[random_row_indices]\n            tree = self._build_tree(subset_X, subset_y)\n\n            # Add each trained tree to the list of models\n            models.append(tree)\n\n        # Save all the trained trees\n        self.trees = models\n\n\nrf_regressor = RandomForestRegressor(10)\nplot_3d_regression(rf_regressor, \"Random Forest Regressor\", \"linear\")\nplot_3d_regression(rf_regressor, \"Random Forest Regressor\", \"ugly\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "main.html#gradient-boosting-regression",
    "href": "main.html#gradient-boosting-regression",
    "title": "Linear Regression",
    "section": "Gradient Boosting Regression",
    "text": "Gradient Boosting Regression\n\nclass GradientBoostingRegressor:\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n\n        # Initialize model with given number of estimators, learning rate, and max depth for each tree\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n\n    def fit(self, X, y):\n        # Initialize predictions as the mean of the target values\n        self.y_pred = np.full_like(y, np.mean(y), dtype=np.float32)\n        self.trees = []\n\n        for _ in range(self.n_estimators):\n            # Calculate residuals\n            residuals = y - self.y_pred\n\n            # Fit a tree to the residuals\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residuals)\n\n            # Update predictions with the tree's output scaled by learning rate\n            self.y_pred += self.learning_rate * tree.predict(X)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        # Start with initial predictions as the mean of target values\n        y_pred = np.full(X.shape[0], np.mean(self.y_pred), dtype=np.float32)\n\n        # Sum the residual predictions from all trees\n        for tree in self.trees:\n            y_pred += self.learning_rate * tree.predict(X)\n        return y_pred\n\n\nboost_regressor = GradientBoostingRegressor(n_estimators=50)\nplot_3d_regression(boost_regressor, \"Gradient Boost Regressor\", \"linear\")\nplot_3d_regression(boost_regressor, \"Gradient Boost Regressor\", \"ugly\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  }
]