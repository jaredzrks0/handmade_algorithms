[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statistics import mean\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport plotly.io as pio\n\n# Use pio to ensure plotly plots render in Quarto\npio.renderers.default = 'notebook_connected'\nHelper Functions\nimport plotly.graph_objects as go\n\ndef plot_3d_regression(model, model_name, data_type = 'linear'):\n\n    # Make Standard Data\n    X, y = make_regression(\n    n_samples=300,     # Number of samples\n    n_features=2,      # Number of features\n    noise=15,          # Add noise to make it realistic\n    random_state=42    # Set seed for reproducibility\n)\n\n    # Make standard non-linear data\n    if data_type.lower() == 'non-linear':\n        y = y + 500 * np.sin(X[:, 0]) + 250 * np.cos(X[:, 1])\n\n    # Make non-standard data\n    if data_type == 'ugly':\n        # Add non-linear transformations\n        y = y + 500 * np.sin(X[:, 0]) * np.cos(X[:, 1]) \\\n              + 300 * (X[:, 0] ** 2) \\\n              - 200 * np.exp(-X[:, 1] ** 2)\n\n        # Add random feature interactions\n        y = y + 100 * (X[:, 0] * X[:, 1])\n\n        # Add random noise to make it \"uglier\"\n        y = y + np.random.normal(0, 150, size=y.shape)\n\n    model.fit(X, y)\n\n\n    # Create a mesh grid for the features\n    x_grid, y_grid = np.meshgrid(np.linspace(min(X[:, 0]), max(X[:, 0]), 100),\n                                np.linspace(min(X[:, 1]), max(X[:, 1]), 100))\n    grid = np.vstack((x_grid.flatten(), y_grid.flatten())).T \n\n    predictions = model.predict(grid)\n\n    # Create 3D scatter plot for training data\n    scatter = go.Scatter3d(\n        x=X[:, 0], y=X[:, 1], z=y,\n        mode='markers', marker=dict(color='blue', size=5), name='Training Data'\n    )\n\n    # Create 3D surface plot for the regression surface\n    surface = go.Surface(\n        x=x_grid, y=y_grid, z=predictions.reshape(x_grid.shape), opacity=0.5, colorscale='Viridis', name='Regression Surface'\n    )\n\n    # Combine both traces into one figure\n    fig = go.Figure(data=[scatter, surface])\n\n    # Update layout for better visualization\n    fig.update_layout(\n        title=f'Training Data and Regression Surface for {model_name}',\n        scene=dict(\n            xaxis_title='Feature 1',\n            yaxis_title='Feature 2',\n            zaxis_title='Target'\n        )\n    )\n\n    # Show plot\n    fig.show()"
  },
  {
    "objectID": "main.html#ols",
    "href": "main.html#ols",
    "title": "Linear Regression",
    "section": "OLS",
    "text": "OLS\n\nclass ols_regression():\n\n    # Initialize the class\n    def __init__(self):\n        pass       \n    \n    def fit(self, X, y):\n        '''Fit the regression to the X data via the OLS equation'''\n\n        # Add a leading colums of 1s to the X data to account for the bias term\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n\n        # Train the data on (X.T @ X)^(-1) @ X.T @ y\n        ols = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n        self.coef = ols[1:]\n        self.bias = ols[0]\n\n    def predict(self, X):\n        '''Predict new data with the trained coefficients and bias'''\n\n        # Check if the X data is 1D and reshape if needed\n        if X.ndim == 1:\n                    X = X.reshape(-1, 1) \n\n        # Make predictions by dotting the new data with the coefficients and adding the bias\n        self.predictions = X.dot(self.coef) + self.bias\n        \n        return self.predictions\n\n\nols = ols_regression()\n\nplot_3d_regression(ols, model_name='OLS', data_type='ugly')"
  },
  {
    "objectID": "main.html#gradient-descent-regression",
    "href": "main.html#gradient-descent-regression",
    "title": "Linear Regression",
    "section": "Gradient Descent Regression",
    "text": "Gradient Descent Regression\n\nclass GDRegression():\n    def __init__(self, epochs, eta):\n        '''Initialize the Gradient Descent Regression Class'''\n        self.epochs = epochs\n        self.eta = eta\n\n    def fit(self, X, y, batch_size = \"TBD\"):\n        '''Train the Gradient Descent Regression Class'''\n\n        if batch_size == 'TBD':\n            batch_size = X.shape[0]\n\n\n        # Create random initialization for the bias and coefficients\n        bias = np.random.random()\n        coef = np.random.random(X.shape[1])\n\n        # Iterate through each epoch\n        for iter in range(self.epochs):\n            \n            indices = np.random.choice(X.shape[0], size=min(batch_size, len(y)), replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n\n            # Make predictions for the X data being trained on\n            y_hat = X_batch.dot(coef) + bias\n\n            # Calculate the derrivative WRT bias and coef given the predicions\n            derr_b = 2/X_batch.shape[0] * sum((y_hat - y_batch))\n            derr_c = 2/X_batch.shape[0] * X_batch.T.dot(y_hat - y_batch)\n\n            # Update the bias and the coef based on the derrivative\n            bias = bias - derr_b * self.eta\n            coef = coef - derr_c * self.eta\n\n        # Finalize the bias and coef\n        self.bias = bias\n        self.coef = coef\n\n    def predict(self, X):\n        '''Predict new data given the learned bias and coef'''\n        predictions = X.dot(self.coef) + self.bias\n        return predictions\n\n        \n\n\ngd_reg = GDRegression(epochs=10000, eta=.01)\nplot_3d_regression(gd_reg, 'Gradient Descent', data_type='ugly')"
  },
  {
    "objectID": "main.html#knn-regression",
    "href": "main.html#knn-regression",
    "title": "Linear Regression",
    "section": "KNN Regression",
    "text": "KNN Regression\n\nclass KNNRegressor():\n    def __init__(self, n_neighbors=5):\n        '''Initialize the regressor with a defined number of nearest neighbors'''\n        self.n_neighbors = n_neighbors\n\n    def fit(self, X, y):\n        '''Train the regressor by loading in all X and y data'''\n        self.X = X\n        self.y = y\n\n    def predict(self, X):\n        '''Make predictions based on the training data using euclidian distance'''\n        predictions = np.empty(0)\n\n        # For each test point...\n        for test_point in X:\n            # Calculate the distance between the test point and all training points\n            distances = np.linalg.norm(self.X - test_point, axis=1)\n\n            # Find the n_neighbors closest points\n            closest_points_indices = np.argsort(distances)[:self.n_neighbors]\n\n            # Use the mean of the closest points to formulate a predictions and append to the predictions array\n            prediction = mean(self.y[closest_points_indices])\n            predictions = np.append(predictions, prediction)\n\n        return predictions\n\n\nknn_regressor = KNNRegressor()\n\nplot_3d_regression(knn_regressor, \"K-Nearest Neighbors Regression\", data_type='ugly')\n\n                                                \n\n\n\nclass DecisionTreeRegressor():\n    def __init__(self, max_depth=None):\n        self.max_depth = max_depth\n\n    # Function for calculating the MSE of a split\n    def mse(self, y):\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    # Function to find the best split at any given point (based on MSE)\n    def _best_split(self, X, y):\n        self.best_mse = float('inf')\n        self.best_feature = None\n        self.best_split_value = None\n        self.best_left_y = None\n        self.best_right_y = None\n\n        for feature_num in range(X.shape[1]):\n            feature_values = np.unique(X[:, feature_num])\n            for value in feature_values:\n                left_index = X[:, feature_num] &lt;= value\n                right_index = X[:, feature_num] &gt; value\n\n                left_targets = y[left_index]\n                right_targets = y[right_index]\n\n                if len(left_targets) &gt; 0 and len(right_targets) &gt; 0:\n                    left_mse = self.mse(left_targets)\n                    right_mse = self.mse(right_targets)\n                    total_average_mse = left_mse * len(left_targets)/len(y) + right_mse * len(right_targets)/len(y)\n\n                    if total_average_mse &lt; self.best_mse:\n                        self.best_mse = total_average_mse\n                        self.best_feature = feature_num\n                        self.best_split_value = value\n                        self.left_y = left_targets\n                        self.right_y = right_targets\n\n        return self.best_split_value, self.best_feature, self.left_y, self.right_y\n    \n    def _build_tree(self, X, y, depth=0):\n        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth):\n            return np.mean(y)\n        \n        best_split_value, best_feature, left_y, right_y = self._best_split(X, y)\n\n        if best_feature == None:\n            return np.mean(y)\n        \n        left_index = X[:, best_feature] &lt;= best_split_value\n        right_index = X[:, best_feature] &gt; best_split_value\n\n        left_tree = self._build_tree(X[left_index], left_y)\n        right_tree = self._build_tree(X[right_index], right_y)\n\n        return {\n            'feature': best_feature,\n            'value': best_split_value,\n            'left': left_tree,\n            'right': right_tree\n        }\n    \n    def _single_prediction(self, tree, x):\n        if isinstance(tree, dict):\n            if x[tree['feature']] &lt; tree['value']:\n                return self._single_prediction(tree['left'], x)\n            else:\n                return self._single_prediction(tree['right'], x)\n        else:\n            return tree\n        \n    def predict(self, X):\n        predictions = np.array([self._single_prediction(self.tree, x) for x in X])\n        return predictions\n\n    def fit(self, X, y):\n        self.tree = self._build_tree(X, y)\n\n\ndt_reg = DecisionTreeRegressor()\nplot_3d_regression(dt_reg, \"Decision Tree\", data_type='ugly')"
  },
  {
    "objectID": "main.html#random-forest-regression",
    "href": "main.html#random-forest-regression",
    "title": "Linear Regression",
    "section": "Random Forest Regression",
    "text": "Random Forest Regression\n\nclass RandomForestRegressor():\n    def __init__(self, n_estimators, max_depth = None):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n\n    # Function for calculating the MSE of a split\n    def mse(self, y):\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    # Function to find the best split at any given point (based on MSE)\n    def _best_split(self, X, y):\n        self.best_mse = float('inf')\n        self.best_feature = None\n        self.best_split_value = None\n        self.best_left_y = None\n        self.best_right_y = None\n\n        # Randomly sample 1/3 of the total features\n        n_features_to_consider = max(1, X.shape[1] // 3)\n        random_feature_indices = np.random.choice(X.shape[1], size=n_features_to_consider, replace=False)\n\n        # Only consider these sampled features for splitting\n        feature_subset = X[:, random_feature_indices]\n\n        for i, feature_num in enumerate(random_feature_indices):  # Map back to original feature indices\n            feature_values = np.unique(feature_subset[:, i])\n            for value in feature_values:\n                # Boolean indices based on the feature subset\n                left_index = feature_subset[:, i] &lt;= value\n                right_index = feature_subset[:, i] &gt; value\n\n                # Subset targets using these indices\n                left_targets = y[left_index]\n                right_targets = y[right_index]\n\n                # Ensure both sides have samples\n                if len(left_targets) &gt; 0 and len(right_targets) &gt; 0:\n                    left_mse = self.mse(left_targets)\n                    right_mse = self.mse(right_targets)\n                    total_average_mse = left_mse * len(left_targets) / len(y) + right_mse * len(right_targets) / len(y)\n\n                    if total_average_mse &lt; self.best_mse:\n                        self.best_mse = total_average_mse\n                        self.best_feature = feature_num\n                        self.best_split_value = value\n                        self.left_y = left_targets\n                        self.right_y = right_targets\n\n        return self.best_split_value, self.best_feature, self.left_y, self.right_y\n\n    \n    def _build_tree(self, X, y, depth=0):\n        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth):\n            return np.mean(y)\n        \n        best_split_value, best_feature, left_y, right_y = self._best_split(X, y)\n\n        if best_feature == None:\n            return np.mean(y)\n        \n        left_index = X[:, best_feature] &lt;= best_split_value\n        right_index = X[:, best_feature] &gt; best_split_value\n\n        left_tree = self._build_tree(X[left_index], left_y)\n        right_tree = self._build_tree(X[right_index], right_y)\n\n        return {\n            'feature': best_feature,\n            'value': best_split_value,\n            'left': left_tree,\n            'right': right_tree\n        }\n\n    def _single_prediction(self, tree, x):\n        if isinstance(tree, dict):\n            if x[tree['feature']] &lt; tree['value']:\n                return self._single_prediction(tree['left'], x)\n            else:\n                return self._single_prediction(tree['right'], x)\n        else:\n            return tree\n        \n    def predict(self, X):\n        predictions = []\n        for tree in self.trees:\n            model_predictions = np.array([self._single_prediction(tree, x) for x in X])\n            predictions.append(model_predictions)\n    \n\n        # Combine all the predictions of all the models and average across the models\n        all_predictions = np.column_stack(predictions)\n        all_predictions = np.mean(all_predictions, axis=1)\n\n        return all_predictions\n        \n\n    def fit(self, X, y):\n        models = []\n        # Create n decision trees, trained with the feature selection and bootstrapping\n        for n in range(self.n_estimators):\n            random_row_indices = np.random.choice(len(X), len(X), replace=True)\n            subset_X = X[random_row_indices]\n            subset_y = y[random_row_indices]\n            tree = self._build_tree(subset_X, subset_y)\n\n            # Add each model to storage\n            models.append(tree)\n\n        # Save all the trained mini-trees\n        self.trees = models\n        \n\n\nrf_regressor = RandomForestRegressor(10)\nplot_3d_regression(rf_regressor, \"Random Forest Regressor\", \"ugly\")"
  },
  {
    "objectID": "main.html#gradient-boosting-regression",
    "href": "main.html#gradient-boosting-regression",
    "title": "Linear Regression",
    "section": "Gradient Boosting Regression",
    "text": "Gradient Boosting Regression\n\nclass GradientBoostRegressor():\n    '''Class for Gradient Boosting Regression. This is a simplification, which utilizes ScikitLearn's DT class without\n       chanaging the default hyperparameters, and also does not include early stopping, instead using all n_estimators'''\n    def __init__(self, n_estimators = 100, models=None, learning_rate=0.1):\n        self.n_estimators = n_estimators\n        self.models = models\n        self.learning_rate = learning_rate\n\n    def fit(self, X, y):\n        initial_predictions = np.full(y.shape, y.mean)\n        self.current_gradient = -(y - initial_predictions)\n\n        # Run through the weak learners, training on the residuals and updating\n        for i in range(self.n_estimators):\n            weak_learner = DecisionTreeRegressor()\n            weak_residual_predictions = weak_learner.fit(X, self.current_errors).predict(X)"
  }
]